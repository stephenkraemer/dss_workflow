{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DML parquet output: currently not possible from within the workflow\n",
    "(see comments on rule). Files are still present in metadata table.\n",
    "Use utils.dml_test_results_to_parquet to create parquet files,\n",
    "from env with r-arrow (not the workflow env, which is anyway automatically\n",
    "created and handled by the workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- conda env with\n",
    "  - snakemake\n",
    "  - pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "git clone https://github.com/stephenkraemer/dss_workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install package with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip -e /path/to/dss_workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# in a python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dss_workflow\n",
    "import snakemake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "specify the conda prefix dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snakemake_conda_prefix = \"/path/to/envs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a config dict, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "dmr_calling_config = {\n",
    "\n",
    "    # Inputs\n",
    "    # ======\n",
    "    # you need a metadata table specifying where your input files are\n",
    "    # look at the example here\n",
    "    \"metadata_table\":      \"/icgc/dkfzlsdf/analysis/hs_ontogeny/results/wgbs/metadata/hierarchy/meth-calls_v1_bistro-0.2.0_odcf-alignment_datafreeze-1.tsv\",\n",
    "    # Required are columns specifying the group (eg 'Tumor'), the sample_id (eg 'Patient1') and the path to methylation calls in BED format, split by chromosome\n",
    "    # BED files are BED6 + beta_value n_meth n_total, with a header line\n",
    "    # See here for an example:\n",
    "    # /icgc/dkfzlsdf/analysis/hs_ontogeny/results/wgbs/results_pmcalls_b-cells_1_CG_chrom-merged_strands-merged.bed.gz\n",
    "    \"group_column\":        \"subject\",\n",
    "    \"sample_id_column\":    \"sample_id\",\n",
    "    \"bed_by_chrom_column\": \"bed_by_chrom_path\",\n",
    "    # Simple BED3 CpG index file, no header\n",
    "    # See example - these are just the coordinates used in all the methylation\n",
    "    # calling BEDs\n",
    "    \"cpg_index_file\": \"/icgc/dkfzlsdf/analysis/hs_ontogeny/results/wgbs/cohort_results/methylation_calling/indices/GRCm38mm10-phix-lambda/GRCm38mm10-phix-lambda_CG_strands-merged.bed.gz\",\n",
    "\n",
    "    # Outputs\n",
    "    # =======\n",
    "    # all results are placed within here\n",
    "    \"output_dir\": \"/icgc/dkfzlsdf/analysis/hs_ontogeny/results/wgbs/cohort_results/pairwise-dmr-calls/DSS/v1_bistro-0.2.0_odcf-alignment/ds1\",\n",
    "    # Optionally create coverage bedgraph - hasn't been run in a while\n",
    "    # not sure if still functional\n",
    "    \"create_dmr_coverage_bedgraph\": False,\n",
    "\n",
    "    # Tasks\n",
    "    # =====\n",
    "    # will only run on specified chromosomes\n",
    "    \"chromosomes\": [\n",
    "        \"1\",\n",
    "        \"10\",\n",
    "        \"11\",\n",
    "        \"12\",\n",
    "        \"13\",\n",
    "        \"14\",\n",
    "        \"15\",\n",
    "        \"16\",\n",
    "        \"17\",\n",
    "        \"18\",\n",
    "        \"19\",\n",
    "        \"2\",\n",
    "        \"3\",\n",
    "        \"4\",\n",
    "        \"5\",\n",
    "        \"6\",\n",
    "        \"7\",\n",
    "        \"8\",\n",
    "        \"9\",\n",
    "    ],\n",
    "    # The pairwise comparisons you want, (group1, group2)\n",
    "    \"comparisons\": [\n",
    "        (\"hsc\", \"mpp1\"),\n",
    "        (\"hsc\", \"mpp5\"),\n",
    "        # ...\n",
    "    ],\n",
    "    # The parameter sets you want to screen\n",
    "    \"parameters\": [\n",
    "        {\n",
    "            \"pvalue\": 0.01,\n",
    "            \"delta\": 0.1,\n",
    "            \"minlen\": 50,\n",
    "            \"minCG\": 2,\n",
    "            \"merge_dist\": 50,\n",
    "            \"pct_sign\": 0.5,\n",
    "            \"smooth\": False,\n",
    "            \"smooth_span\": 500,\n",
    "        },\n",
    "        {\n",
    "            \"pvalue\": 0.001,\n",
    "            \"delta\": 0.1,\n",
    "            \"minlen\": 50,\n",
    "            \"minCG\": 2,\n",
    "            \"merge_dist\": 50,\n",
    "            \"pct_sign\": 0.5,\n",
    "            \"smooth\": False,\n",
    "            \"smooth_span\": 500,\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run snakemake; this should work directly on the DKFZ LSF cluster using the supplied clutster interaction scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note\n",
    "- that we use the python package to find all resource files\n",
    "- that we can allow multiple restarts, at each restart, the requested resources will be enlarged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snakemake.snakemake(\n",
    "    snakefile=dss_workflow.get_snakefile_path(),\n",
    "    latency_wait=60,\n",
    "    config=dmr_calling_config,\n",
    "    nodes=5000,\n",
    "    max_jobs_per_second=10,\n",
    "    jobscript=dss_workflow.get_jobscript_path(),\n",
    "    cluster=dss_workflow.get_submitscript_path(),\n",
    "    cluster_status=dss_workflow.get_statusscript_path(),\n",
    "    dryrun=True,\n",
    "    conda_prefix=snakemake_conda_prefix,\n",
    "    use_conda=True,\n",
    "    restart_times=4,\n",
    "    keepgoing=True,\n",
    "    # may be necessary if run was aborted\n",
    "    # force_incomplete=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, create a record of all files that were just produced as metadata table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_metadata_table = dss_workflow.create_dmr_metadata_table(\n",
    "    dmr_calling_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that parquet files are currently not automatically produced, see impl. notes above. Parquet files are handy for collaboration with R users."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:light",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
